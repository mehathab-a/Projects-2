{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382d89f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 09:17:43.669131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 09:17:43.738856: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-15 09:17:43.757778: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 09:17:44.098388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mehathab-pc/miniconda3/envs/tensor/lib/\n",
      "2022-11-15 09:17:44.098429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mehathab-pc/miniconda3/envs/tensor/lib/\n",
      "2022-11-15 09:17:44.098433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg19 import VGG19,preprocess_input\n",
    "from Segmentation import *\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from warnings import filterwarnings\n",
    "from skimage.transform import rescale\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c69c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_function(img):\n",
    "    \n",
    "    # Convert to BGR image\n",
    "    img_bgr = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Removing Shadows from the Images    \n",
    "#     img_rgb_plane = cv2.split(img_bgr) # Spliting the color channels\n",
    "#     result_norm_planes = []\n",
    "#     for plane in img_rgb_plane:\n",
    "#         plane = plane.astype(\"uint8\")\n",
    "#         dilated_img = cv2.dilate(plane, np.ones((25, 25), np.uint8)) # dilate image\n",
    "#         bg_img = cv2.medianBlur(dilated_img, 85)  # blur background\n",
    "#         diff_img = 255 - cv2.absdiff(plane, bg_img)  # find the difference in the image pixels\n",
    "#         norm_img = cv2.normalize(diff_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "#         result_norm_planes.append(norm_img)\n",
    "#     result_norm = cv2.merge(result_norm_planes)  # Merging all normalized color channels\n",
    "\n",
    "    blank_mask = np.zeros(img_bgr.shape, dtype=np.uint8)\n",
    "    original = img_bgr.copy()\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array([18, 42, 69])\n",
    "    higher = np.array([179,255,255])\n",
    "    cnts = cv2.findContours(img_bgr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "    for c in cnts:\n",
    "        cv2.drawContours(blank_mask,[c], -1, (255,255,255), -1)\n",
    "        break\n",
    "\n",
    "    result = cv2.bitwise_and(original,blank_mask)\n",
    "\n",
    "    cv2.imshow('result', result)\n",
    "                    \n",
    "    \n",
    "    # Convert the Normalized image to GrayScale\n",
    "    image_grayscale = cv2.cvtColor(result_norm,cv2.COLOR_BGR2GRAY).astype(\"uint8\")\n",
    "    \n",
    "    # Equalize Image for exposure Equalization\n",
    "    image_equalize = cv2.equalizeHist(image_grayscale)\n",
    "\n",
    "    # Blur Image using Gaussian Filter\n",
    "    image_blur = cv2.GaussianBlur(image_equalize,(15,15),0)\n",
    "    image_blur = image_blur.astype(\"uint8\")\n",
    "    \n",
    "    # Impliment Otsu's Threshold - Detect Boundaries\n",
    "    res,image_threshold = cv2.threshold(image_blur,0,255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Perform Morphological Transform to eliminate holes within the image regions\n",
    "    kernel = np.ones((50,50), np.uint8)\n",
    "    image_morph = cv2.morphologyEx(image_threshold,cv2.MORPH_OPEN, kernel)\n",
    "    image_morph = cv2.cvtColor(image_morph,cv2.COLOR_GRAY2RGB) \n",
    "    \n",
    "    # Smoothing edges:\n",
    "    kernel = np.ones((15,15), np.uint8)\n",
    "    image_smooth = cv2.morphologyEx(image_morph, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Merge Orginal Image with masked area\n",
    "    image = cv2.cvtColor(img_bgr,cv2.COLOR_BGR2RGB).astype(\"uint8\")\n",
    "    image_bitwise = cv2.bitwise_or(image_smooth,image)\n",
    "    \n",
    "#     # Preprocess Image if Pretrained Model used.\n",
    "    image_bitwise = preprocess_input(image_bitwise)\n",
    "    \n",
    "    return image_bitwise.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479e63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = ImageDataGenerator(\n",
    "                                   featurewise_center=True,\n",
    "                                   featurewise_std_normalization=True,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   rotation_range=10,\n",
    "                                   shear_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   fill_mode='nearest',\n",
    "                                   preprocessing_function=preprocessing_function,\n",
    "#                                    rescale =1./255\n",
    "                                    )\n",
    "\n",
    "validation_generator = ImageDataGenerator(preprocessing_function=preprocessing_function,\n",
    "                                         rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d5ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "target_size = (224,224)\n",
    "class_mode = 'categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67970 images belonging to 32 classes.\n",
      "Found 15435 images belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "path_train = 'New_Dataset_plant/Orginal/train/'\n",
    "path_valid = 'New_Dataset_plant/Orginal/valid/'\n",
    "\n",
    "training_data = training_generator.flow_from_directory(path_train,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      target_size=target_size,\n",
    "                                                      class_mode=class_mode,\n",
    "                                                      shuffle=True)\n",
    "\n",
    "validating_data = validation_generator.flow_from_directory(path_valid,\n",
    "                                                          batch_size = batch_size,\n",
    "                                                          target_size = target_size,\n",
    "                                                          class_mode = class_mode,\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "641ad34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /io/opencv/modules/imgproc/src/contours.cpp:195: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_img, labels \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dicte \u001b[38;5;241m=\u001b[39m training_data\u001b[38;5;241m.\u001b[39mclass_indices\n\u001b[1;32m      3\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dicte\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.9/site-packages/keras/preprocessing/image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.9/site-packages/keras/preprocessing/image.py:385\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    383\u001b[0m         params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mget_random_transform(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    384\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mapply_transform(x, params)\n\u001b[0;32m--> 385\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     batch_x[i] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# optionally save augmented images to disk for debugging purposes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor/lib/python3.9/site-packages/keras/preprocessing/image.py:1851\u001b[0m, in \u001b[0;36mImageDataGenerator.standardize\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;124;03m\"\"\"Applies the normalization configuration in-place to a batch of\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;124;03minputs.\u001b[39;00m\n\u001b[1;32m   1835\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;124;03m    The inputs, normalized.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_function:\n\u001b[0;32m-> 1851\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale:\n\u001b[1;32m   1853\u001b[0m     x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale\n",
      "Cell \u001b[0;32mIn [15], line 23\u001b[0m, in \u001b[0;36mpreprocessing_function\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     21\u001b[0m lower \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m69\u001b[39m])\n\u001b[1;32m     22\u001b[0m higher \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m179\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m cnts \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindContours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRETR_EXTERNAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHAIN_APPROX_SIMPLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m cnts \u001b[38;5;241m=\u001b[39m cnts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cnts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cnts[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m cnts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(cnts, key\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcontourArea, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/contours.cpp:195: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n"
     ]
    }
   ],
   "source": [
    "train_img, labels = training_data.next()\n",
    "dicte = training_data.class_indices\n",
    "classes = list(dicte.keys())\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d1d4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_img(img, label):\n",
    "    for i,imgs in enumerate(img):\n",
    "        image = imgs\n",
    "        pos = np.argmax(label[i])\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(image)\n",
    "        plt.title(classes[pos])\n",
    "        plt.show()\n",
    "    \n",
    "plot_img(train_img,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d102ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12279d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
